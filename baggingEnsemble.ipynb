{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6b1f854",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T04:02:32.680307Z",
     "start_time": "2023-08-05T04:02:30.239742Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.nn.utils import weight_norm\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, recall_score\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "from optuna.trial import TrialState\n",
    "import optuna\n",
    "from random import *\n",
    "import copy\n",
    "import gc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8162877",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T04:02:32.805113Z",
     "start_time": "2023-08-05T04:02:32.681088Z"
    }
   },
   "outputs": [],
   "source": [
    "# Seed\n",
    "import random\n",
    "seed = 2023\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "776cd606",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T04:02:32.809908Z",
     "start_time": "2023-08-05T04:02:32.806333Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len = 6): # model dim = 60, max_len = 5\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model).float() # 5*60\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1) # 5*1\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp() # ([0,2,...,58] * -0.9...)^2\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # 偶數維度sin\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # 奇數維度cos\n",
    "\n",
    "        pe = pe.unsqueeze(0) # 1*5*60\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "\n",
    "    def forward(self, x): # x = batch * max_len * d_model\n",
    "        x = x + self.pe[:, :x.size(1)].clone().detach().requires_grad_(False)\n",
    "        return  x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d8218c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T04:02:32.824628Z",
     "start_time": "2023-08-05T04:02:32.810607Z"
    }
   },
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, query, key, value, mask = None, dropout = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            `query`: shape (batch_size, n_heads, max_len, d_q)\n",
    "            `key`: shape (batch_size, n_heads, max_len, d_k)\n",
    "            `value`: shape (batch_size, n_heads, max_len, d_v)\n",
    "            `mask`: shape (batch_size, 1, 1, max_len)\n",
    "            `dropout`: nn.Dropout\n",
    "        Returns:\n",
    "            `weighted value`: shape (batch_size, n_heads, max_len, d_v)\n",
    "            `weight matrix`: shape (batch_size, n_heads, max_len, max_len)\n",
    "        \"\"\"\n",
    "        d_k = query.size(-1)  # d_k = d_model / n_heads\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)  # B*H*L*L\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask.eq(1), -1e20)\n",
    "        p_attn = F.softmax(scores, dim=-1)  # B*H*L*L\n",
    "        \n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "        \n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, dropout):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.h = n_heads\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.sdpa = ScaledDotProductAttention()\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            `query`: shape (batch_size, max_len, d_model)\n",
    "            `key`: shape (batch_size, max_len, d_model)\n",
    "            `value`: shape (batch_size, max_len, d_model)\n",
    "            `mask`: shape (batch_size, max_len)\n",
    "        \n",
    "        Returns:\n",
    "            shape (batch_size, max_len, d_model)\n",
    "        \"\"\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads. B*1*1*L\n",
    "            mask = mask.unsqueeze(1).unsqueeze(1)\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2) for l, x in\n",
    "                             zip(self.linears, (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        # x: B x H x L x D_v\n",
    "        x, self.attn = self.sdpa(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20fe1f65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T04:02:32.843892Z",
     "start_time": "2023-08-05T04:02:32.825879Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_ff, max_len, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.positional_embedding = PositionalEmbedding(d_model, max_len)\n",
    "        \n",
    "        self.block1 = TransformerEncoderBlock(n_heads, d_model, d_ff, dropout)\n",
    "#         self.block2 = TransformerEncoderBlock(n_heads, d_model, d_ff, dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.linear1 = nn.Linear(60, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "                \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.positional_embedding(x)\n",
    "        \n",
    "        x = self.block1(x, mask)\n",
    "#         x = self.block2(x, mask)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        a = self.linear1(x[:, -1, :])\n",
    "        out = self.sigmoid(a)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_ff, dropout=0.1):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.multihead_attention = MultiHeadAttention(n_heads, d_model, dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attended = self.multihead_attention(x, x, x, mask=mask)\n",
    "        residual1 = x + self.dropout(attended)\n",
    "        norm1_output = self.norm1(residual1)\n",
    "\n",
    "        feed_forward_output = self.feed_forward(norm1_output)\n",
    "        residual2 = norm1_output + self.dropout(feed_forward_output)\n",
    "        norm2_output = self.norm2(residual2)\n",
    "\n",
    "        return norm2_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f259068",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T04:02:33.133825Z",
     "start_time": "2023-08-05T04:02:32.845215Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('1_60_standardized.csv',low_memory=False,index_col=0)\n",
    "df23 = pd.read_csv('2_3_60_standardized.csv',low_memory=False,index_col=0)\n",
    "df45 = pd.read_csv('4_5_60_standardized.csv',low_memory=False,index_col=0)\n",
    "check_for_any_nan= df45.isna().values.any()\n",
    "print(check_for_any_nan)\n",
    "check_for_nan = df45.isna().sum().sum()\n",
    "print(check_for_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73400987",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T04:02:33.137969Z",
     "start_time": "2023-08-05T04:02:33.134564Z"
    }
   },
   "outputs": [],
   "source": [
    "def dftransform(data, lenth):\n",
    "    # lenth時間最大長度\n",
    "    if lenth !=1:\n",
    "        people_list = list(data[\"病歷號\"].unique())\n",
    "        d = dict.fromkeys(people_list, 0)\n",
    "        for index,row in data.iterrows():\n",
    "            if data.loc[index,\"病歷號\"] in people_list:\n",
    "                d[data.loc[index,\"病歷號\"]] += 1 # d will show how many visits a person have\n",
    "        # we got a csv with two years \n",
    "        # we try to split them into different csv\n",
    "        P = []    \n",
    "        p = []\n",
    "        Count = 0\n",
    "        count = 0\n",
    "        for j,k in d.items():\n",
    "            if k == lenth:\n",
    "                Count += 1\n",
    "                P.append(j)\n",
    "            else:\n",
    "                count += 1\n",
    "                p.append(j)\n",
    "\n",
    "        a = data[data['病歷號'].isin(p)]\n",
    "        b = data[data['病歷號'].isin(P)]\n",
    "\n",
    "        return a, b, p, P # a b is all of that year's columns, p P is the person list \n",
    "    \n",
    "    else:\n",
    "        people_list = list(data[\"病歷號\"].unique())\n",
    "        d = dict.fromkeys(people_list, 0)\n",
    "        for index,row in data.iterrows():\n",
    "            if data.loc[index,\"病歷號\"] in people_list:\n",
    "                d[data.loc[index,\"病歷號\"]] += 1\n",
    " \n",
    "        p = []\n",
    "        count = 0\n",
    "        for j,k in d.items():\n",
    "                count += 1\n",
    "                p.append(j)\n",
    "\n",
    "        a = data[data['病歷號'].isin(p)]\n",
    "\n",
    "\n",
    "        return a, p # a is all of that year's columns, p is the person list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f19c8209",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T04:02:35.242327Z",
     "start_time": "2023-08-05T04:02:33.138826Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_df1, p1 = dftransform(df1, 1)\n",
    "filtered_df2, filtered_df3, p2, p3 = dftransform(df23, 3)\n",
    "filtered_df4, filtered_df5, p4, p5 = dftransform(df45, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31087a0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T04:06:38.934497Z",
     "start_time": "2023-08-05T04:06:38.927794Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_features(filtered_df, p): # p : person list\n",
    "    features = []\n",
    "    targets = []\n",
    "    group = filtered_df.groupby(\"病歷號\")\n",
    "    for i in p: # i : each person\n",
    "        count = 0 \n",
    "        age = []\n",
    "        temp_df  = group.get_group(i)\n",
    "        data = []\n",
    "        for index, row in temp_df.iterrows():\n",
    "            if data == []:\n",
    "                for ln in range(5):\n",
    "                    age.append(int(temp_df.loc[index, '年齡']+ln))\n",
    "            \n",
    "            for l in range(len(age)):\n",
    "                if int(temp_df.loc[index, '年齡']) == age[count]:\n",
    "                    count+=1\n",
    "                    data.append(list(temp_df.loc[index][2:]))\n",
    "                    break\n",
    "                else:\n",
    "                    count+=1\n",
    "                    data.append([0]*60)# padding 0\n",
    "                    \n",
    "        max_len = 5 - count \n",
    "        if max_len != 0:\n",
    "            for m in range(max_len):\n",
    "                data.insert(0,[0]*60)\n",
    "        data.append([1]*60)\n",
    "        \n",
    "        features.append(data)\n",
    "        targets.append([temp_df.iloc[0][0]])\n",
    "    targets = np.array(targets,np.dtype(np.float32))\n",
    "    features = np.array(features,np.dtype(np.float32))\n",
    "\n",
    "    return targets, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2545c9e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T04:06:47.110087Z",
     "start_time": "2023-08-05T04:06:39.315427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1282, 1)\n",
      "(1282, 6, 60)\n",
      "(1156, 1)\n",
      "(1156, 6, 60)\n",
      "(1021, 1)\n",
      "(1021, 6, 60)\n",
      "(1394, 1)\n",
      "(1394, 6, 60)\n",
      "(3800, 1)\n",
      "(3800, 6, 60)\n"
     ]
    }
   ],
   "source": [
    "targets1, features1 = get_features(filtered_df1, p1)\n",
    "targets2, features2 = get_features(filtered_df2, p2)\n",
    "targets3, features3 = get_features(filtered_df3, p3)\n",
    "targets4, features4 = get_features(filtered_df4, p4)\n",
    "targets5, features5 = get_features(filtered_df5, p5)\n",
    "print(targets1.shape)\n",
    "print(features1.shape)\n",
    "print(targets2.shape)\n",
    "print(features2.shape)\n",
    "print(targets3.shape)\n",
    "print(features3.shape)\n",
    "print(targets4.shape)\n",
    "print(features4.shape)\n",
    "print(targets5.shape)\n",
    "print(features5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c0754a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-28T12:42:08.267515Z",
     "start_time": "2023-07-28T12:42:08.265971Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_tensor(tensor):\n",
    "    tensor = np.array(tensor)\n",
    "    tensor = torch.from_numpy(tensor)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc75561a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-28T12:42:08.295379Z",
     "start_time": "2023-07-28T12:42:08.268271Z"
    }
   },
   "outputs": [],
   "source": [
    "targets1 = to_tensor(targets1)\n",
    "targets2 = to_tensor(targets2)\n",
    "targets3 = to_tensor(targets3)\n",
    "targets4 = to_tensor(targets4)\n",
    "targets5 = to_tensor(targets5)\n",
    "features1 = to_tensor(features1)\n",
    "features2 = to_tensor(features2)\n",
    "features3 = to_tensor(features3)\n",
    "features4 = to_tensor(features4)\n",
    "features5 = to_tensor(features5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "483ce4a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-28T12:42:08.325371Z",
     "start_time": "2023-07-28T12:42:08.296524Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_dataset(X, y, test_size=0.2, val_size=0.5, random_state=seed):\n",
    "    \"\"\"\n",
    "    將資料集拆分為訓練集、驗證集和測試集。\n",
    "\n",
    "    參數：\n",
    "    - X：特徵資料\n",
    "    - y：目標變數\n",
    "    - test_size：測試集的比例（預設為0.2）\n",
    "    - val_size：驗證集的比例（預設為0.2）\n",
    "    - random_state：隨機種子（預設為None）\n",
    "\n",
    "    返回值：\n",
    "    - X_train, X_val, X_test：拆分後的特徵資料（訓練集、驗證集、測試集）\n",
    "    - y_train, y_val, y_test：拆分後的目標變數（訓練集、驗證集、測試集）\n",
    "    \"\"\"\n",
    "    # 先拆分出測試集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify = y)\n",
    "    \n",
    "    \n",
    "    # 再從剩餘的資料中拆分出驗證集\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=random_state, stratify = y_test)\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86b2046c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-28T12:42:08.382460Z",
     "start_time": "2023-07-28T12:42:08.329598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練集1大小: 1025\n",
      "驗證集1大小: 129\n",
      "測試集1大小: 128\n",
      "訓練集2大小: 924\n",
      "驗證集2大小: 116\n",
      "測試集2大小: 116\n",
      "訓練集3大小: 816\n",
      "驗證集3大小: 103\n",
      "測試集3大小: 102\n",
      "訓練集4大小: 1115\n",
      "驗證集4大小: 140\n",
      "測試集4大小: 139\n",
      "訓練集5大小: 3040\n",
      "驗證集5大小: 380\n",
      "測試集5大小: 380\n"
     ]
    }
   ],
   "source": [
    "datasets = [(features1, targets1), (features2, targets2), (features3, targets3), (features4, targets4), (features5, targets5)]\n",
    "train_size = 0.8\n",
    "val_size = 0.5\n",
    "\n",
    "x_train = []\n",
    "x_val = []\n",
    "x_test = []\n",
    "y_train = []\n",
    "y_val = []\n",
    "y_test = []\n",
    "\n",
    "for i, (features, targets) in enumerate(datasets):\n",
    "    x_train_i, x_val_i, x_test_i, y_train_i, y_val_i, y_test_i = split_dataset(features, targets, \n",
    "                                                                               test_size=(1-train_size), \n",
    "                                                                               val_size=val_size, random_state=seed)\n",
    "    x_train.append(x_train_i)\n",
    "    x_val.append(x_val_i)\n",
    "    x_test.append(x_test_i)\n",
    "    y_train.append(y_train_i)\n",
    "    y_val.append(y_val_i)\n",
    "    y_test.append(y_test_i)\n",
    "    print(f\"訓練集{i+1}大小:\", len(x_train_i))\n",
    "    print(f\"驗證集{i+1}大小:\", len(x_val_i))\n",
    "    print(f\"測試集{i+1}大小:\", len(x_test_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86fc66f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-28T12:42:08.389019Z",
     "start_time": "2023-07-28T12:42:08.383307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "整合後的訓練集大小: 6920 6920\n",
      "整合後的測試集大小: 865 865\n",
      "整合後的驗證集大小: 868 868\n"
     ]
    }
   ],
   "source": [
    "x_train_combined = np.concatenate(x_train, axis=0)\n",
    "x_test_combined = np.concatenate(x_test, axis=0)\n",
    "x_val_combined = np.concatenate(x_val, axis=0)\n",
    "\n",
    "y_train_combined = np.concatenate(y_train, axis=0)\n",
    "y_test_combined = np.concatenate(y_test, axis=0)\n",
    "y_val_combined = np.concatenate(y_val, axis=0)\n",
    "\n",
    "print(\"整合後的訓練集大小:\", len(x_train_combined), len(y_train_combined))\n",
    "print(\"整合後的測試集大小:\", len(x_test_combined), len(y_test_combined))\n",
    "print(\"整合後的驗證集大小:\", len(x_val_combined), len(y_val_combined))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f28e227e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-28T12:42:08.410559Z",
     "start_time": "2023-07-28T12:42:08.389749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 的數量: 1008\n",
      "0 的數量: 5912\n"
     ]
    }
   ],
   "source": [
    "num_ones = np.sum(y_train_combined == 1)\n",
    "num_zeros = np.sum(y_train_combined == 0)\n",
    "\n",
    "# 印出結果\n",
    "print(\"1 的數量:\", num_ones)\n",
    "print(\"0 的數量:\", num_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b41a7722",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-28T12:42:08.430409Z",
     "start_time": "2023-07-28T12:42:08.411579Z"
    }
   },
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import BorderlineSMOTE\n",
    "# n_samples, n_time_steps, n_features = x_train_combined.shape\n",
    "# x_train_combined = x_train_combined.reshape(n_samples, n_features * n_time_steps)\n",
    "# smote = BorderlineSMOTE(random_state = 2023, sampling_strategy = 1)\n",
    "# x_train_oversampled, y_train_oversampled = smote.fit_resample(x_train_combined, y_train_combined)\n",
    "# x_train_oversampled = x_train_oversampled.reshape(-1, n_time_steps, n_features)\n",
    "# y_train_oversampled = y_train_oversampled.reshape(y_train_oversampled.shape[0],1)\n",
    "# # 確認過採樣後的訓練集大小\n",
    "# print(\"過採樣後的訓練集大小:\", x_train_oversampled.shape, y_train_oversampled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5281496a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-28T12:42:08.454779Z",
     "start_time": "2023-07-28T12:42:08.433750Z"
    }
   },
   "outputs": [],
   "source": [
    "# num_ones = np.sum(y_train_oversampled == 1)\n",
    "# num_zeros = np.sum(y_train_oversampled == 0)\n",
    "\n",
    "# # 印出結果\n",
    "# print(\"1 的數量:\", num_ones)\n",
    "# print(\"0 的數量:\", num_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e78d3657",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-28T12:42:08.474080Z",
     "start_time": "2023-07-28T12:42:08.458597Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_mask(seq, size):\n",
    "    data = torch.empty(size, 6) # CLS.ver\n",
    "    for i in range(len(seq)):\n",
    "        for j in range(len(seq[i])):\n",
    "            if seq[i, j, 1] == 0:\n",
    "                data[i, j] = 1 #遮起來\n",
    "            else:\n",
    "                data[i, j] = 0#不遮起來\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48bff041",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-28T12:42:08.704514Z",
     "start_time": "2023-07-28T12:42:08.478305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6920\n",
      "865\n",
      "868\n"
     ]
    }
   ],
   "source": [
    "x_train_mask = make_mask(x_train_combined, x_train_combined.shape[0])\n",
    "x_test_mask = make_mask(x_test_combined, x_test_combined.shape[0])\n",
    "x_val_mask = make_mask(x_val_combined, x_val_combined.shape[0])\n",
    "print(len(x_train_mask))\n",
    "print(len(x_test_mask))\n",
    "print(len(x_val_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c15cdb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-28T12:42:08.709045Z",
     "start_time": "2023-07-28T12:42:08.705248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6920"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ind=[]\n",
    "test_ind=[]\n",
    "val_ind=[]\n",
    "\n",
    "for i in range(len(x_train_combined)):\n",
    "    train_ind.append(i)\n",
    "for i in range(len(x_test_combined)):\n",
    "    test_ind.append(i)\n",
    "for i in range(len(x_val_combined)):\n",
    "    val_ind.append(i)\n",
    "\n",
    "len(train_ind)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd3e8bcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-28T12:42:08.726011Z",
     "start_time": "2023-07-28T12:42:08.709703Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, features, targets, mask, index):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        self.masks = mask\n",
    "        self.indexs = index\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx], self.masks[idx], self.indexs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0770ee21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-28T12:42:08.752516Z",
     "start_time": "2023-07-28T12:42:08.726753Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(x_train_combined, y_train_combined, x_train_mask, train_ind)\n",
    "test_dataset = MyDataset(x_test_combined, y_test_combined, x_test_mask, test_ind)\n",
    "val_dataset = MyDataset(x_val_combined, y_val_combined, x_val_mask, val_ind)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "              batch_size=8868,\n",
    "              shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "              batch_size=865,\n",
    "              shuffle=False)\n",
    "val_loader = DataLoader(dataset=val_dataset,\n",
    "              batch_size=868,\n",
    "              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d88802d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-28T12:42:08.864304Z",
     "start_time": "2023-07-28T12:42:08.755207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6920, 6, 60]) torch.Size([6920, 1]) torch.Size([6920, 6]) torch.Size([6920])\n"
     ]
    }
   ],
   "source": [
    "for X, Y, M, I in (train_loader):\n",
    "    print(X.shape, Y.shape, M.shape, I.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4c683fe5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-28T14:22:40.359672Z",
     "start_time": "2023-07-28T14:22:40.352709Z"
    }
   },
   "outputs": [],
   "source": [
    "def TrainModel(model, loss_fn, optimizer, train_loader, test_loader, epochs, patience):\n",
    "    best_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        t_losses = []\n",
    "        v_losses = []\n",
    "        \n",
    "        for X, Y, M, I in train_loader:\n",
    "            X = X.to(torch.float32).to(device)\n",
    "            M = M.to(torch.float32).to(device)\n",
    "            Y = Y.to(torch.float32).to(device)\n",
    "            \n",
    "            Y_preds = model(X, M)\n",
    "            loss = loss_fn(Y_preds, Y)\n",
    "            t_losses.append(loss.item())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, Y, M, I in test_loader:\n",
    "                X = X.to(torch.float32).to(device)\n",
    "                M = M.to(torch.float32).to(device)\n",
    "                Y = Y.to(torch.float32).to(device)\n",
    "                \n",
    "                \n",
    "                Y_preds = model(X, M)\n",
    "                loss = loss_fn(Y_preds, Y)\n",
    "                v_losses.append(loss.item())\n",
    "        \n",
    "        avg_t_loss = sum(t_losses) / len(t_losses)\n",
    "        avg_v_loss = sum(v_losses) / len(v_losses)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_t_loss:.5f}, Validation Loss: {avg_v_loss:.5f}\")\n",
    "\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_v_loss < best_loss:\n",
    "            best_loss = avg_v_loss\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(\"Early stopping triggered. No improvement in validation loss.\")\n",
    "                break\n",
    "    \n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8f3f62d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-28T14:22:41.767031Z",
     "start_time": "2023-07-28T14:22:41.760661Z"
    }
   },
   "outputs": [],
   "source": [
    "def MakePredictions(model, loader):\n",
    "    Y_shuffled, Y_preds = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad(): \n",
    "        for X, Y, M,I in (loader):\n",
    "            X=X.to(torch.float32).to(device)\n",
    "            M=M.to(torch.float32).to(device)\n",
    "            Y=Y.to(torch.float32).to(device)\n",
    "            preds = model(X, M)\n",
    "            preds=preds.gt(0.5).int()\n",
    "            Y_preds.append(preds)\n",
    "            Y_shuffled.append(Y)\n",
    "        gc.collect()\n",
    "        Y_preds, Y_shuffled = torch.cat(Y_preds), torch.cat(Y_shuffled)\n",
    "\n",
    "        return Y_shuffled.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c7fd1be3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-28T14:22:43.491059Z",
     "start_time": "2023-07-28T14:22:43.464639Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2016, 6, 60]) torch.Size([2016, 1]) torch.Size([2016, 6]) torch.Size([2016])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class BalancedDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.labels = np.array([label.item() for _, label, _, _ in self.dataset])  # 將標籤轉換成可雜湊的型別\n",
    "        self.num_samples = len(self.labels)\n",
    "        self.class_counts = Counter(self.labels)\n",
    "        self.minority_class = min(self.class_counts, key=self.class_counts.get)\n",
    "        self.majority_class = max(self.class_counts, key=self.class_counts.get)\n",
    "        self.num_minority_samples = self.class_counts[self.minority_class]\n",
    "        self.num_majority_samples = self.class_counts[self.majority_class]\n",
    "        self.balance_indices()\n",
    "\n",
    "    def balance_indices(self):\n",
    "        majority_indices = np.where(self.labels == self.majority_class)[0]\n",
    "        minority_indices = np.where(self.labels == self.minority_class)[0]\n",
    "        balanced_majority_indices = np.random.choice(majority_indices, self.num_minority_samples, replace=False)\n",
    "        balanced_indices = np.concatenate([balanced_majority_indices, minority_indices])\n",
    "        np.random.shuffle(balanced_indices)\n",
    "        self.balanced_indices = balanced_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.balanced_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        original_idx = self.balanced_indices[idx]\n",
    "        return self.dataset[original_idx]\n",
    "# 建立平衡的訓練資料集\n",
    "balanced_train_dataset = BalancedDataset(train_dataset)\n",
    "\n",
    "# 使用平衡後的資料集建立 DataLoader\n",
    "balanced_train_loader = DataLoader(dataset=balanced_train_dataset, batch_size=8868, shuffle=True)\n",
    "\n",
    "for X, Y, M, I in (balanced_train_loader):\n",
    "    print(X.shape, Y.shape, M.shape, I.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "50f0fef8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-29T05:31:04.023136Z",
     "start_time": "2023-07-29T05:31:03.998768Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 基底模型的數量\n",
    "num_base_models = 1\n",
    "\n",
    "base_models = []\n",
    "for i in range(num_base_models):\n",
    "    base_model = TransformerEncoder(n_heads=4, d_model=60, d_ff=120, max_len=6).to(device)\n",
    "    base_models.append(base_model)\n",
    "    \n",
    "for i, base_model in enumerate(base_models):\n",
    "    print(f'model_{i}')\n",
    "    balanced_train_dataset = BalancedDataset(train_dataset)\n",
    "    balanced_train_loader = DataLoader(dataset=balanced_train_dataset, batch_size=2016, shuffle=True)\n",
    "\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = torch.optim.AdamW(base_model.parameters(), lr=0.001)\n",
    "\n",
    "    epochs = 250\n",
    "    patience = 100\n",
    "    \n",
    "    TrainModel(base_model, loss_fn, optimizer, balanced_train_loader, val_loader, epochs, patience)\n",
    "\n",
    "    torch.save(base_model.state_dict(), f\"base_model_{i}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "48c43c48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-29T05:31:04.933568Z",
     "start_time": "2023-07-29T05:31:04.830759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(868, 1)\n",
      "Test Accuracy : 0.7638248847926268\n",
      "\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         沒確診       0.96      0.75      0.84       740\n",
      "          確診       0.37      0.83      0.51       128\n",
      "\n",
      "    accuracy                           0.76       868\n",
      "   macro avg       0.66      0.79      0.68       868\n",
      "weighted avg       0.87      0.76      0.80       868\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      "[[557 183]\n",
      " [ 22 106]]\n"
     ]
    }
   ],
   "source": [
    "# 加載已訓練的基底模型並進行預測\n",
    "predictions = []\n",
    "for i in range(num_base_models):\n",
    "    base_model = TransformerEncoder(n_heads=4, d_model=60, d_ff=120, max_len=6).to(device)\n",
    "    base_model.load_state_dict(torch.load(f\"base_model_{i}.pt\"))\n",
    "    base_model.eval()\n",
    "\n",
    "    # 進行預測\n",
    "    Y_shuffled, Y_preds = MakePredictions(base_model, val_loader)\n",
    "    predictions.append(Y_preds)\n",
    "\n",
    "# 對預測結果進行投票\n",
    "ensemble_predictions = np.mean(predictions, axis=0)\n",
    "final_predictions = (ensemble_predictions > 0.5).astype(int)\n",
    "\n",
    "print(final_predictions.shape)\n",
    "\n",
    "target_classes = ['沒確診','確診']\n",
    "print(\"Test Accuracy : {}\".format(accuracy_score(Y_shuffled, final_predictions)))\n",
    "print(\"\\nClassification Report : \")\n",
    "print(classification_report(Y_shuffled, final_predictions, target_names=target_classes))\n",
    "print(\"\\nConfusion Matrix : \")\n",
    "print(confusion_matrix(Y_shuffled, final_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e8a4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
